{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://sparkbyexamples.com/pyspark/python-no-module-named-pyspark-error/\n",
    "#### https://spark.apache.org/docs/latest/sql-data-sources-json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark_json_test1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark_json_test1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24b6d866be0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load json file of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------+--------------------+-------+\n",
      "|           client_id|fidaty_card|  name|       products_list|surname|\n",
      "+--------------------+-----------+------+--------------------+-------+\n",
      "|b3667dab-fead-438...|       True|giulia|[{Rizzoli, filett...|bianchi|\n",
      "|1422cc9e-5c31-436...|       True| mario|[{Taylors of Harr...|foscolo|\n",
      "+--------------------+-----------+------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiline_df = spark.read.option(\"multiline\",\"true\").option(\"inferSchema\",\"true\") \\\n",
    "      .json(\"C:\\Anna\\Data science\\Big Data\\Esselunga project\\\\big-data\\Transactions_example3.json\")\n",
    "multiline_df.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- fidaty_card: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- products_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |    |    |-- upc: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multiline_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       products_list|\n",
      "+--------------------+\n",
      "|[{Rizzoli, filett...|\n",
      "|[{Taylors of Harr...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Making a query test using spark commands\n",
    "multiline_df.select('products_list').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------+--------------------+-------+--------------------+\n",
      "|           client_id|fidaty_card|  name|       products_list|surname|        products_new|\n",
      "+--------------------+-----------+------+--------------------+-------+--------------------+\n",
      "|b3667dab-fead-438...|       True|giulia|[{Rizzoli, filett...|bianchi|{Rizzoli, filetti...|\n",
      "|b3667dab-fead-438...|       True|giulia|[{Rizzoli, filett...|bianchi|{Barilla, mezze p...|\n",
      "|b3667dab-fead-438...|       True|giulia|[{Rizzoli, filett...|bianchi|{Rizzoli, filetti...|\n",
      "|1422cc9e-5c31-436...|       True| mario|[{Taylors of Harr...|foscolo|{Taylors of Harro...|\n",
      "|1422cc9e-5c31-436...|       True| mario|[{Taylors of Harr...|foscolo|{Taylors of Harro...|\n",
      "|1422cc9e-5c31-436...|       True| mario|[{Taylors of Harr...|foscolo|{Barilla, mezze p...|\n",
      "+--------------------+-----------+------+--------------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer\n",
    "exploding = multiline_df.withColumn(\"products_new\", explode_outer(\"products_list\"))\n",
    "#multiline_df.select(multiline_df.products_list,explode(array(multiline_df.products_list))).show()\n",
    "exploding.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- fidaty_card: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- products_list: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- description: string (nullable = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |    |    |-- upc: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- products_new: struct (nullable = true)\n",
      " |    |-- description: string (nullable = true)\n",
      " |    |-- price: double (nullable = true)\n",
      " |    |-- quantity: long (nullable = true)\n",
      " |    |-- upc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exploding.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, NumericType\n",
    "schema = StructType([ \n",
    "    StructField(\"upc\",StringType(),True), \n",
    "    StructField(\"description\",StringType(),True), \n",
    "    StructField(\"price\",NumericType(),True), \n",
    "    StructField(\"quantity\", NumericType(), True)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+-----------------+\n",
      "|         description|price|quantity|              upc|\n",
      "+--------------------+-----+--------+-----------------+\n",
      "|Rizzoli, filetti ...| 2.29|       2|2078-878573272326|\n",
      "|Barilla, mezze pe...| 0.85|       3|2568-745573272456|\n",
      "|Rizzoli, filetti ...| 2.29|       2|2078-878573272326|\n",
      "|Taylors of Harrog...| 3.79|       3|2529-338608111118|\n",
      "|Taylors of Harrog...| 3.79|       4|2529-338608111118|\n",
      "|Barilla, mezze pe...| 0.85|       5|2568-745573272456|\n",
      "+--------------------+-----+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table = exploding.select('products_new.*')\n",
    "new_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raggruppo per upc e sommo le quantità vendute. La quantità venduta per ogni prodotto è la daily sales velocity, che entra poi nel calcolo del ROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- upc: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- daily sales velocity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rop_indicators = new_table.groupBy('upc','description').sum('quantity').withColumnRenamed('sum(quantity)','daily sales velocity')\n",
    "rop_indicators.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggiungo una colonna per il ROP iniziale, una per lo stock level, una per il lead time (espresso in giorni) e una per la reorder quantity. Tutti questi valori sono fittizi e si possono poi sostituire con quelli inseriti dell'inventory di ogni retailer (o del supplier, a seconda che si stia ricalcolando il ROP per il singolo retailer o per il supplier centrale).\n",
    "\n",
    "#### rand() genera un numero random da 0 a 1, perciò l'ho moltiplicato per 10. Con floor arrotondo per difetto all'intero più vicino e infine sommo 1 per evitare di avere valori pari a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+---+-----------+---------+----------------+\n",
      "|              upc|         description|daily sales velocity|rop|stock level|lead time|reorder quantity|\n",
      "+-----------------+--------------------+--------------------+---+-----------+---------+----------------+\n",
      "|2568-745573272456|Barilla, mezze pe...|                   8| 10|          9|        2|               7|\n",
      "|2529-338608111118|Taylors of Harrog...|                   7|  5|          7|        2|               3|\n",
      "|2078-878573272326|Rizzoli, filetti ...|                   4|  2|          8|        2|               9|\n",
      "+-----------------+--------------------+--------------------+---+-----------+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, floor, rand\n",
    "\n",
    "rop_indicators = rop_indicators.withColumn(\"rop\", (floor(rand()*10))+1) #rop iniziale\n",
    "rop_indicators = rop_indicators.withColumn(\"stock level\", (floor(rand()*10))+1)\n",
    "rop_indicators = rop_indicators.withColumn(\"lead time\", (floor(rand()*2))+1)\n",
    "rop_indicators = rop_indicators.withColumn(\"reorder quantity\", (floor(rand()*10))+1)\n",
    "rop_indicators.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo una colonna 'new rop' in cui inserisco il nuovo valore del rop se esso si discosta per più del 10% (al rialzo o al ribasso) dal rop precedente oppure il vecchio rop se tale scostamento è inferiore al 10%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-----------+---------+----------------+---+-------+\n",
      "|              upc|         description|daily sales velocity|stock level|lead time|reorder quantity|rop|new rop|\n",
      "+-----------------+--------------------+--------------------+-----------+---------+----------------+---+-------+\n",
      "|2568-745573272456|Barilla, mezze pe...|                   8|          9|        2|               7| 25|     16|\n",
      "|2529-338608111118|Taylors of Harrog...|                   7|          7|        2|               3| 21|     14|\n",
      "|2078-878573272326|Rizzoli, filetti ...|                   4|          8|        2|               9| 16|      8|\n",
      "+-----------------+--------------------+--------------------+-----------+---------+----------------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "rop_indicators = rop_indicators.withColumn(\n",
    "    \"new rop\", when((rop_indicators['daily sales velocity']*rop_indicators['lead time'])>(1.10*rop_indicators['rop']), (rop_indicators['daily sales velocity']*rop_indicators['lead time']))\n",
    "    .when((rop_indicators['daily sales velocity']*rop_indicators['lead time'])<(0.90*rop_indicators['rop']), (rop_indicators['daily sales velocity']*rop_indicators['lead time']))\n",
    "    .otherwise(rop_indicators['rop'])\n",
    "    )\n",
    "rop_indicators.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elimino la vecchia colonna rop e rinomino la nuova, così poi si può mettere lo script in un ciclo che viene eseguito periodicamente una volta al giorno a fine giornata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-----------+---------+----------------+---+\n",
      "|              upc|         description|daily sales velocity|stock level|lead time|reorder quantity|rop|\n",
      "+-----------------+--------------------+--------------------+-----------+---------+----------------+---+\n",
      "|2568-745573272456|Barilla, mezze pe...|                   8|          9|        2|               7| 16|\n",
      "|2529-338608111118|Taylors of Harrog...|                   7|          7|        2|               3| 14|\n",
      "|2078-878573272326|Rizzoli, filetti ...|                   4|          8|        2|               9|  8|\n",
      "+-----------------+--------------------+--------------------+-----------+---------+----------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rop_indicators = rop_indicators.drop('rop').withColumnRenamed('new rop', 'rop')\n",
    "rop_indicators.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5a060a823ebfc0a46fed09e5d4d49b6552270a7dac258bcadf0308cb8ded9d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
